import fs from "fs/promises";
import path from "path";
import cron from "node-cron";
import sources from "./config/rssSources.js";
import { scrapeRSS } from "./scrapers/rssScraper.js";

const ALL_NEWS_FILE = path.resolve("./data/all_news.json");
const LAST_DATES_FILE = path.resolve("./data/last_dates.json");

let isFetching = false;

// Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø­Ù„ÙŠÙ‹Ø§ ÙÙ‚Ø· Ø¥Ù† Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
async function ensureFiles() {
  await fs.mkdir("./data", { recursive: true });

  try {
    await fs.access(ALL_NEWS_FILE);
  } catch {
    await fs.writeFile(ALL_NEWS_FILE, "[]");
  }

  try {
    await fs.access(LAST_DATES_FILE);
  } catch {
    await fs.writeFile(LAST_DATES_FILE, "{}");
  }
}

async function loadJSON(file, fallback) {
  try {
    return JSON.parse(await fs.readFile(file, "utf8"));
  } catch {
    return fallback;
  }
}

async function saveJSON(file, data) {
  await fs.writeFile(file, JSON.stringify(data, null, 2), "utf8");
}

async function fetchAll() {

  if (isFetching) {
    console.log("â³ Previous fetch still running, skipping this cycle...");
    return;
  }

  isFetching = true;

  try {
    await ensureFiles();

    let allNews = await loadJSON(ALL_NEWS_FILE, []);
    let lastDates = await loadJSON(LAST_DATES_FILE, {});

    const beforeCount = allNews.length; // âœ… Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¶Ø§ÙØ©

    console.log("\n====================================================");
    console.log(`ğŸ•’ Fetch cycle started: ${new Date().toISOString()}`);
    console.log(`ğŸ“‚ Items BEFORE this cycle: ${beforeCount}`);
    console.log("====================================================");

    let totalNew = 0;

    for (const source of sources) {
      console.log(`\nğŸŒ Fetching from: ${source.name}`);

      const items = await scrapeRSS(source);

      console.log(`ğŸ“¥ Extracted: ${items.length} items from ${source.name}`);

      const lastSourceDate =
        lastDates[source.name] || "1970-01-01T00:00:00Z";

      let added = 0;

      for (const item of items) {
        if (!item.date) continue;

        const itemDate = new Date(item.date).getTime();
        const lastDate = new Date(lastSourceDate).getTime();

        if (itemDate > lastDate) {
          allNews.push(item);
          added++;
          totalNew++;

          if (!lastDates[source.name] || item.date > lastDates[source.name]) {
            lastDates[source.name] = item.date;
          }
        }
      }

      console.log(`âœ… New items added from ${source.name}: ${added}`);
      console.log(`ğŸ“Š Total news count so far: ${allNews.length}`);
    }

    await saveJSON(ALL_NEWS_FILE, allNews);
    await saveJSON(LAST_DATES_FILE, lastDates);

    const afterCount = allNews.length; // âœ… Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø¶Ø§ÙØ©

    console.log("\n====================================================");
    console.log(`ğŸ“‚ Items BEFORE this cycle: ${beforeCount}`);
    console.log(`ğŸ†• New items added this cycle: ${totalNew}`);
    console.log(`ğŸ“¦ Items AFTER this cycle: ${afterCount}`);
    console.log(`âœ… Check: before + new = ${beforeCount + totalNew}`);
    console.log("====================================================\n");

  } catch (err) {
    console.error("âŒ Fetch cycle failed:", err);
  } finally {
    isFetching = false;
  }
}


// ØªØ´ØºÙŠÙ„ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
fetchAll();

// ØªØ´ØºÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙƒÙ„ 3 Ø¯Ù‚Ø§Ø¦Ù‚ Ù…Ø­Ù„ÙŠÙ‹Ø§ ÙÙ‚Ø·
cron.schedule("*/3 * * * *", fetchAll);
console.log("âœ… RSS Fetcher running locally every 3 minutes");
